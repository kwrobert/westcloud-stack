#####################################################################################################
# Script: compute_node_cluster.yaml
# Author: Kyle Robertson
# Date: September 18, 2015
# Description: A HEAT template that deploys an autoscaling cluster of servers that scales up or down
# based on CPU usage. Each server is part of a TORQUE pbs job scheduler and has all necessary services
# to run TORQUE job scheduling pre-installed. 
#####################################################################################################

heat_template_version: 2018-08-31
description: Homogenous compute cluster

parameters:
  max_cluster_size:
    type: number
    description: Maximum number of servers in autoscaling cluster
    constraints:
      - range: {min: 1, max: 8}
    default: 8
  min_cluster_size:
    type: number
    description: Minimum number of servers in autoscaling cluster
    constraints:
      - range: {min: 1, max: 2}
    default: 2
  image:
    type: string
    label: Image name or ID
    description: Bootable image used to boot all instances
    default: CentOS 7
  flavor:
    type: string
    label: Instance flavor
    description: Type of flavor that defines virtual resources for instance
    constraints:
      - allowed_values: 
        - p4-6gb       
        - c8-60gb-186  
        - p8-12gb      
        - c2-7.5gb-31  
        - p2-3gb       
        - c8-90gb-186  
        - p1-1.5gb     
        - c4-30gb-83   
        - c1-7.5gb-30  
        - c16-120gb-392
        - c4-15gb-83   
        - c2-15gb-31   
        - c16-60gb-392 
        - c16-90gb-392 
        - c8-30gb-186  
        - c4-45gb-83   
    default: p1-1.5gb
  ssh_key:
    type: string
    label: Key name
    description: Name for SSH keypair used to access instances
    default: westcloud
  internal_net:
    type: string
    label: Internal tenant network name or ID
    description: Tenant network to attach compute nodes to
  sec_group:
    type: string
    label: Compute Node Security Group
    description: Security group for compute nodes to give them access to one another and to headnodes
  compute_node_data_volume_size:
    type: number
    description: Size of the persistent data volume in GB
    
resources:
  compute_cluster:
    type: OS::Heat::AutoScalingGroup
    properties:
      max_size: {get_param: max_cluster_size}
      desired_capacity: 2
      min_size: {get_param: min_cluster_size} 
      resource:
        type: compute_node.yaml
        properties:
          flavor: { get_param: flavor }
          image: { get_param: image }
          key_name: { get_param: ssh_key }
          network: {get_param: internal_net} 
          metadata: {"metering.stack": {get_param: "OS::stack_id"}}
          sec_group: {get_param: sec_group}
          data_volume_size: {get_param: compute_node_data_volume_size}
          
  compute_cluster_scaleup_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: compute_cluster}
      cooldown: 60
      scaling_adjustment: 1

  compute_cluster_scaledown_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: {get_resource: compute_cluster}
      cooldown: 60
      scaling_adjustment: -1

  # cpu_alarm_high:
  #   type: OS::Ceilometer::Alarm
  #   properties:
  #     description: Scale-up if the average CPU > 80% for 1 minute
  #     meter_name: cpu_util
  #     statistic: avg
  #     period: 60
  #     evaluation_periods: 1
  #     threshold: 80
  #     alarm_actions:
  #       - {get_attr: [compute_cluster_scaleup_policy, alarm_url]}
  #     matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
  #     comparison_operator: gt

  # cpu_alarm_low:
  #   type: OS::Ceilometer::Alarm
  #   properties:
  #     description: Scale-down if the average CPU < 20% for 60 minutes
  #     meter_name: cpu_util
  #     statistic: avg
  #     period: 60
  #     evaluation_periods: 1
  #     threshold: 20
  #     alarm_actions:
  #       - {get_attr: [compute_cluster_scaledown_policy, alarm_url]}
  #     matching_metadata: {'metadata.user_metadata.stack': {get_param: "OS::stack_id"}}
  #     comparison_operator: lt
      
outputs:
  server_ips:
    description: List of IP addresses of the compute servers
    value: {get_attr: [compute_cluster, outputs_list, networks, {get_param: internal_net}, 0]} 
  scale_up_url:
    value: {get_attr: [compute_cluster_scaleup_policy, alarm_url]}
  scale_down_url:
    value: {get_attr: [compute_cluster_scaledown_policy, alarm_url]}
  scale_up_signal_url:
    value: {get_attr: [compute_cluster_scaleup_policy, signal_url]}
  scale_down_signal_url:
    value: {get_attr: [compute_cluster_scaledown_policy, signal_url]}
  scale_up_info:
    value: {get_attr: [compute_cluster_scaleup_policy, show]}
  scale_down_info:
    value: {get_attr: [compute_cluster_scaledown_policy, show]}
